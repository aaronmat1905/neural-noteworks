{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOXaGDQrP7mWugGf2kbaIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronmat1905/neural-noteworks/blob/main/AllAboutAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention**\n",
        "> Attention is a Scheme for calculating Similarity between words\n",
        "\n",
        "*And in the case of NLP, this can be used as weights to capture **Contextual Meaning** as weighted average*\n",
        "\n",
        "Here, we are going to explore these kinds of Attention:\n",
        "- Self Attention\n",
        "- Causal Attention\n",
        "- Cross Attention\n",
        "- Co- Attention\n",
        "- Multi-Head Attention"
      ],
      "metadata": {
        "id": "rYYq0jOt5SXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "SVZ2DRux5SAR",
        "outputId": "fef43e55-0cef-43e0-b579-8c75a19eccf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e805eedfb30>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Sentence**: *The Professor who supervised the student published the paper*"
      ],
      "metadata": {
        "id": "J01eTpwRCuyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The Professor who supervised the student published the paper\"\n",
        "tokens = sentence.split(\" \")\n",
        "\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "inputs = torch.tensor([\n",
        "    [0.92, 0.05, 0.03, 0.02, 0.01, 0.00],  # The\n",
        "    [0.04, 0.88, 0.10, 0.76, 0.05, 0.02],  # professor\n",
        "    [0.02, 0.04, 0.05, 0.03, 0.01, 0.91],  # who\n",
        "    [0.03, 0.15, 0.87, 0.72, 0.06, 0.65],  # supervised\n",
        "    [0.90, 0.06, 0.02, 0.01, 0.01, 0.00],  # the\n",
        "    [0.05, 0.82, 0.12, 0.69, 0.04, 0.03],  # student\n",
        "    [0.02, 0.10, 0.91, 0.78, 0.07, 0.08],  # published\n",
        "    [0.91, 0.04, 0.03, 0.02, 0.01, 0.00],  # the\n",
        "    [0.03, 0.06, 0.04, 0.81, 0.89, 0.02],  # paper\n",
        "], dtype=torch.float32)\n",
        "\n",
        "\n",
        "print(inputs.shape)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "QFVIAwESCuk8",
        "outputId": "c7a52070-5a4b-4518-d309-9559a11c0a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'Professor', 'who', 'supervised', 'the', 'student', 'published', 'the', 'paper']\n",
            "torch.Size([9, 6])\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self Attention**"
      ],
      "metadata": {
        "id": "i63XihQQ4BYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Self-Attention\n",
        "> Focuses on a **Single Sequence** and generates a Contextualized-vector based on the Query.\n",
        "- **Steps**:\n",
        "  1. Dot Product (*Compute Similarity Scores*) (Matrix Form):  $$S=X^TX$$\n",
        "  2. Normalize Scores using softmax: $$A = softmax(S)$$\n",
        "  3. Compute Context Vector: $$Z=AX$$"
      ],
      "metadata": {
        "id": "VS4ocUwVDehm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zVauvBkC3z7o",
        "outputId": "8241d350-370a-4ec5-ea75-85d6e4bd4aff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4874, 0.1526, 0.1296, 0.2335, 0.0755, 0.1214],\n",
            "        [0.2555, 0.4071, 0.2553, 0.5744, 0.1466, 0.1734],\n",
            "        [0.2250, 0.1471, 0.1666, 0.2633, 0.0763, 0.2476],\n",
            "        [0.2502, 0.2930, 0.4736, 0.6419, 0.1536, 0.3486],\n",
            "        [0.4788, 0.1518, 0.1279, 0.2316, 0.0748, 0.1208],\n",
            "        [0.2545, 0.3719, 0.2433, 0.5327, 0.1371, 0.1700],\n",
            "        [0.2449, 0.2792, 0.4173, 0.5992, 0.1541, 0.2438],\n",
            "        [0.4828, 0.1514, 0.1292, 0.2325, 0.0753, 0.1213],\n",
            "        [0.2410, 0.2459, 0.2272, 0.5311, 0.2567, 0.1630]])\n"
          ]
        }
      ],
      "source": [
        "class SelfAttentionNaive(nn.Module):\n",
        "  def forward(self, inputs):\n",
        "    # inputs => [n, d]\n",
        "    attention_scores = inputs @ inputs.T\n",
        "    attention_weights = F.softmax(attention_scores, dim=0)\n",
        "    return attention_weights @ inputs # Multiply it with Input Once More\n",
        "\n",
        "# @ is dot product\n",
        "sfn = SelfAttentionNaive()\n",
        "print(sfn.forward(inputs=inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, in this approach --\n",
        "- No weights are Trained\n",
        "- Order/Proximity of words do not matter\n",
        "- Works for any sequence of Length\n",
        "- Each token **attends** to all other tokens\n",
        "\n",
        "*And, as an Improvement:*\n"
      ],
      "metadata": {
        "id": "1EWDfq_Jh1rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scaled-Dot-Product Self-Attention (QKV)**\n",
        "> Introducing three new Matrices, Q (Query) | K (Key) | V (Value);\n",
        "\n",
        "In Naive Self Attention, the same vector plays 3 simultaneous role; They are logically different, and forcing them to be same is a bottle neck.\n",
        "\n",
        "- Here we multiply the input embedding by each matrix, hence, **Projecting** it into that specialized **Rolespace**\n",
        "\n",
        "### Steps\n",
        "1. Compute Q, K, V Matrices in such a fashion: $$Q = XW_q\\\\K=XW_k\\\\V=XW_v$$\n",
        "2. Compute the Attention **Weights** using the Scaled-Dot product Attention formula: $$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d^k}})V$$\n",
        "3. Dot Product between Input and Attention Weights: $$Attention(Q,K,V) @ Inputs$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8_dsCYtcjEmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    # nn.Linear is equivalent to weights + optional bias\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "  def forward(self, x):\n",
        "    Q = self.W_query(x)\n",
        "    K = self.W_key(x)\n",
        "    V = self.W_value(x)\n",
        "\n",
        "    scores = Q @ K.transpose(1,2)\n",
        "    weights = F.softmax(scores/K.shape[-1]**0.5, dim=1)\n",
        "    return weights @ V\n",
        "\n",
        "\n",
        "sfn2 = SelfAttention(d_in = 6, d_out = 6)\n",
        "print(sfn2.forward(inputs))"
      ],
      "metadata": {
        "id": "Xa6Kcs9LjFir",
        "outputId": "f92d848a-d698-4679-95cc-d5dfcb6cadc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2112, -0.1142, -0.1308,  0.0293, -0.0843, -0.0458],\n",
            "         [ 0.0444, -0.0795,  0.0619,  0.1691, -0.0586, -0.0051],\n",
            "         [ 0.1500, -0.0404,  0.1419,  0.2016, -0.0348,  0.0035],\n",
            "         [-0.0958, -0.0445,  0.1271,  0.2116,  0.0071,  0.0276],\n",
            "         [ 0.0475, -0.0820,  0.0411,  0.1533, -0.0757, -0.0174],\n",
            "         [ 0.1864, -0.0549,  0.0712,  0.1387, -0.0598,  0.0186],\n",
            "         [-0.1015, -0.1121, -0.1165,  0.0348, -0.1056, -0.0435],\n",
            "         [-0.0321, -0.1037, -0.0268,  0.1121, -0.0901, -0.0323],\n",
            "         [ 0.1188, -0.1026,  0.0110,  0.1421, -0.1180, -0.0307]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Causal-Attention**\n",
        "\n",
        "> A form of self-attention in which each token is allowed to attend only to itself and the tokens that come before it in the sequence, while all future tokens are masked out to prevent information leakage during autoregressive generation.\n",
        "\n",
        "Masking is done in order to help in Auto Regressive Generation Tasks."
      ],
      "metadata": {
        "id": "lVnJ3jRa4FWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, d_in]\n",
        "        b, n, _ = x.shape\n",
        "        Q = self.W_query(x)                 # [b, n, d_out]\n",
        "        K = self.W_key(x)                   # [b, n, d_out]\n",
        "        V = self.W_value(x)                 # [b, n, d_out]\n",
        "        scores = Q @ K.transpose(1, 2)      # [b, n, n]\n",
        "        scores.masked_fill_(\n",
        "            self.mask[:n, :n].bool(),\n",
        "            float(\"-inf\")\n",
        "        )\n",
        "        weights = F.softmax(scores/(K.shape[-1] ** 0.5), dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "        return weights @ V                  # [b, n, d_out]\n",
        "inputs = torch.randn(1, 9, 6)   # batch=1, 9 tokens, 6 features\n",
        "cat = CausalAttention(6, 6, context_length=9, dropout=0.2)\n",
        "output = cat(inputs)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "90ZYkbVo5Dqz",
        "outputId": "cbe5d906-276d-4d94-c47b-c900c478c666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0397,  0.5003, -0.5396, -0.2649, -0.2974,  0.8441],\n",
            "         [ 0.0189,  0.2385, -0.2572, -0.1263, -0.1417,  0.4024],\n",
            "         [-0.1345,  0.1765, -0.1833, -0.4175, -0.0259,  0.1651],\n",
            "         [-0.1674, -0.0155, -0.0150, -0.2710,  0.0724, -0.1349],\n",
            "         [ 0.1232, -0.1457, -0.0322, -0.1056,  0.0663, -0.0248],\n",
            "         [ 0.0869, -0.0165,  0.2347, -0.0344,  0.0506, -0.1278],\n",
            "         [-0.0498, -0.2216, -0.0875, -0.1384,  0.1016, -0.1841],\n",
            "         [-0.0275, -0.2030, -0.0009, -0.1318,  0.1208, -0.1890],\n",
            "         [-0.1280, -0.0760, -0.0962, -0.1308,  0.0211, -0.0859]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Co-Attention**"
      ],
      "metadata": {
        "id": "Q0_hIrSb5EHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working..."
      ],
      "metadata": {
        "id": "olpfmrYy5Hjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-Attention**"
      ],
      "metadata": {
        "id": "iY9hhehK5H19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working..."
      ],
      "metadata": {
        "id": "gYDE0cX75LgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Head Attention**"
      ],
      "metadata": {
        "id": "9AV9cWUK5LyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working..."
      ],
      "metadata": {
        "id": "FeS7Qw_KAklc"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}